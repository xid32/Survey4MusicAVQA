# üéº Music Performance Audio-Visual Question Answering Requires Specialized Multimodal Designs

Welcome to the companion repository for our position paper on Music Performance Audio-Visual Question Answering (Music AVQA). This repo curates the datasets, benchmark results, and seminal methods surveyed in the paper, and it will be continuously updated as the field evolves. Whether you are reproducing baselines, exploring bias-reduced or robustness-focused splits, or designing new music-specific architectures, you will find ready-to-use links, code pointers, and concise summaries here. We hope this living resource accelerates research on rich, densely layered audio-visual reasoning in musical contexts and sparks fresh ideas for multimodal music understanding.

## Content

- [Background](#background)
- [Datasets](#datasets)
- [Methods](#methods)
- [Description](#description)

## Background

Music‚Äêperformance scenes span solo performances, ensemble of the same instrument, ensemble of different instruments, and culture-specific groups. Within this setting, Music AVQA questions generally fall into five categories, detailed in the table below.
The models listed in [&sect; Methods](#methods) are commonly evaluated on these question types, while ongoing work must still overcome dense-signal, hierarchical-temporal, and cross-modal challenges using the three main music AVQA benchmarks summarised in [&sect; Datasets](#datasets).

| Question Type | Description                                                                                | Example                                                         |
| ------------- | ------------------------------------------------------------------------------------------ | --------------------------------------------------------------- |
| Existential   | Determine whether an audible event corresponds to a visible object or action in the scene. | ‚ÄúIs the sound coming from the piano shown in the video?‚Äù        |
| Counting      | Estimate the number of audio-visual elements that meet a specified condition.              | ‚ÄúHow many instruments are playing simultaneously?‚Äù              |
| Location      | Identify the spatial position of the sound source within the visual frame.                 | ‚ÄúWhere is the first instrument that starts playing?‚Äù            |
| Comparative   | Compare an attribute of two or more audio-visual elements.                                 | ‚ÄúIs the violin on the left louder than the cello on the right?‚Äù |
| Temporal      | Reason about the order or timing of events across modalities.                              | ‚ÄúWhich instrument plays right after the drum beat?‚Äù             |

## Datasets

The table below tracks the progression of the MUSIC-AVQA datasets from the 2022 debut to later versions that reduce bias and enhance robustness. It includes direct download links, reference papers and concise descriptions of each release.

| Year | Dataset                                                                                                                                          | Paper Title                                                                                                                                                                                                                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2022 | [Music-AVQA](https://drive.google.com/drive/folders/1WAryZZE0srLIZG8VHl22uZ3tpbGHtsrQ)                                                           | [Learning to Answer Questions in Dynamic Audio-Visual Scenarios](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf)                                   | The MUSIC-AVQA dataset represents a significant contribution to audio-visual question answering research, comprising 9,288 videos with over 150 hours of musical performances covering 22 instruments, generating 45,867 question-answer pairs. The dataset is randomly split into training, validation, and testing sets with 32,087, 4,595, and 9,185 QA pairs respectively, spanning 33 question templates across 9 question types including existential, location, counting, comparative, and temporal questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| 2023 | [Music-AVQA-v2.0](https://www.dropbox.com/scl/fi/yfrrsuds95tmtgxirxy6e/collect_new_set.rar?rlkey=izt6ldb1czlpnhy8y6masi2jm&e=1&st=8o3m8ds9&dl=0) | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf) | The MUSIC-AVQA v2.0 dataset builds upon the original MUSIC-AVQA by addressing data bias issues, comprising 10,518 videos (9,288 from the original plus 1,230 new videos) with musical performances covering 22 instruments, generating approximately 54,000 question-answer pairs. The balanced dataset splits into training, validation, and testing sets with 36,700, 5,250, and 10,819 QA pairs respectively, spanning 33 question templates across 9 question types. The authors specifically balance 15 biased templates by ensuring no dominant answers exceed 60\% for binary questions or 50\% for multi-class questions, particularly enhancing representation of underrepresented answers in existential, counting, temporal, location, and comparative question categories.                                                                                                                                                                                                                                              |
| 2024 | [Music-AVQA-R](https://github.com/GeWu-Lab/MUSIC-AVQA/tree/main/data/json_update)                                                                | [Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering](https://proceedings.neurips.cc/paper_files/paper/2024/file/123a18dfd821c8b440f42a00a27648d6-Paper-Conference.pdf)                                             | The MUSIC-AVQA-R dataset proposed in this paper is an extension of MUSIC-AVQA specifically designed to evaluate the robustness of audio-visual question answering models. It expands the original test set through a human-machine collaboration mechanism that rephrases each question 25 times, increasing the number of questions from 9,129 to 211,572, and introduces distribution shifts to categorize questions into head (common) and tail (rare) samples. Compared to the original dataset, MUSIC-AVQA-R features a vocabulary size of 465 (five times larger than MUSIC-AVQA), provides more diverse question formulations while preserving inherent biases in the training and validation sets, and offers three evaluation metrics‚Äîhead accuracy, tail accuracy, and overall accuracy‚Äîenabling researchers to assess model performance in both in-distribution and out-of-distribution scenarios, making it the first dataset specifically designed for robustness evaluation in audio-visual question answering tasks. |

## Methods

The table below summarizes key milestone models in Visual and Audio-Visual QA, listing each method‚Äôs related paper, publication venue and any available code for quick experimentation.

| Year | Method         | Paper Title                                                                                                                                                                                                                                                     | Conference/Journal | Code                                                                |
| ---- | -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------- |
| 2015 | GRU            | [VQA: Visual Question Answering](https://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf)                                                                                                                          | ICCV 15'           | -                                                                   |
| 2016 | Att-BLSTM      | [Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification](https://aclanthology.org/P16-2034.pdf)                                                                                                                              | ACL 16'            | [Github](https://github.com/youngbin-ro/Attention-Based-BiLSTM)     |
| 2019 | AVSD           | [A Simple Baseline for Audio-Visual Scene-Aware Dialog](https://openaccess.thecvf.com/content_CVPR_2019/papers/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.pdf)                                                              | CVPR 19'           | [Github](https://github.com/idansc/simple-avsd)                     |
| 2019 | MCAN           | [Deep Modular Co-Attention Networks for Visual Question Answering](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf)                                              | CVPR 19'           | [Github](https://github.com/MILVLG/mcan-vqa)                        |
| 2019 | PSAC           | [Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering](https://ojs.aaai.org/index.php/AAAI/article/download/4887/4760)                                                                                                         | AAAI 19'           | [Github](https://github.com/lixiangpengcs/PSAC)                     |
| 2019 | HME            | [Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering](https://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.pdf)   | CVPR 19'           | [Github](https://github.com/fanchenyou/HME-VideoQA)                 |
| 2020 | HCRN           | [Hierarchical Conditional Relation Networks for Video Question Answering](https://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.pdf)                                | CVPR 20'           | [Github](https://github.com/thaolmk54/hcrn-videoqa)                 |
| 2021 | LAViT          | [Pano-AVQA: Grounded Audio-Visual Question Answering on 360‚ó¶ Videos](https://hs-yn.github.io/assets/pdf/2021iccv_panoavqa.pdf)                                                                                                                                  | ICCV 21'           | [Github](https://github.com/HS-YN/PanoAVQA)                         |
| 2022 | AVST           | [Learning to Answer Questions in Dynamic Audio-Visual Scenarios](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf)                                                   | CVPR 22'           | [Github](https://github.com/GeWu-Lab/MUSIC-AVQA)                    |
| 2023 | ChatBridge     | [ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst](https://arxiv.org/pdf/2305.16103)                                                                                                                                            | CoRR 23'           | [Github](https://github.com/joez17/ChatBridge)                      |
| 2023 | CIGN           | [Class-Incremental Grouping Network for Continual Audio-Visual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.pdf)                                   | ICCV 23'           | [Github](https://github.com/stoneMo/CIGN)                           |
| 2023 | COCA           | [COCA: COllaborative CAusal Regularization for Audio-Visual Question Answering](https://ojs.aaai.org/index.php/AAAI/article/download/26527/26299)                                                                                                               | AAAI 23'           | -                                                                   |
| 2023 | CONVLSTM       | [Temporal Reasoning via Audio Question Answering](https://aclanthology.org/2023.findings-emnlp.630.pdf)                                                                                                                                                         | EMNLP 23'          | [Github](https://github.com/Bravo5542/TJSTG)                        |
| 2023 | FCNLSTM        | [Temporal Reasoning via Audio Question Answering](https://aclanthology.org/2023.findings-emnlp.630.pdf)                                                                                                                                                         | EMNLP 23'          | [Github](https://github.com/Bravo5542/TJSTG)                        |
| 2023 | DCL            | [Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning](https://proceedings.neurips.cc/paper_files/paper/2023/file/29571f8fda54fe93631c41aad4215abc-Paper-Conference.pdf)                                                         | NIPS 23'           | [Github](https://github.com/Andy20178/DCL)                          |
| 2023 | DG-SCT         | [Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks](https://proceedings.neurips.cc/paper_files/paper/2023/file/af01716e08073368a7c8a62be46dba17-Paper-Conference.pdf)                                                    | NIPS 23'           | [Github](https://github.com/haoyi-duan/DG-SCT)                      |
| 2023 | LAVisH         | [Vision Transformers are Parameter-Efficient Audio-Visual Learners](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf)                                            | CVPR 23'           | [Github](https://github.com/GenjiB/LAVISH)                          |
| 2023 | LSTTA          | [Parameter-Efficient Transfer Learning for Audio-Visual-Language Tasks](https://arxiv.org/pdf/2308.14274)                                                                                                                                                       | MM 23'             | -                                                                   |
| 2023 | PSTP-Net       | [Progressive Spatio-temporal Perception for Audio-Visual Question Answering](https://arxiv.org/pdf/2308.05421)                                                                                                                                                  | MM 23'             | [Github](https://github.com/GeWu-Lab/PSTP-Net)                      |
| 2023 | VAST           | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://arxiv.org/pdf/2305.18500)                                                                                                                                               | NIPS 23'           | [Github](https://github.com/TXH-mercury/VAST)                       |
| 2024 | Amuse          | [Learning Musical Representations for Music Performance Question Answering](https://aclanthology.org/2024.findings-emnlp.159.pdf)                                                                                                                               | EMNLP 24'          | [Github](https://github.com/xid32/Amuse?tab=readme-ov-file)         |
| 2024 | Audio Flamingo | [Audio Flamingo: a novel audio language model with few-shot learning and dialogue abilities](https://arxiv.org/pdf/2402.01831)                                                                                                                                  | ICML 24'           | [Github](https://github.com/NVIDIA/audio-flamingo)                  |
| 2024 | AVMoE          | [Mixture of Experts for Audio-Visual Learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/009729d26288b9a8826023692a876107-Paper-Conference.pdf)                                                                                                | NIPS 24'           | [Github](https://github.com/yingchengy/AVMOE)                       |
| 2024 | AVSiam         | [Siamese Vision Transformers are Scalable Audio-visual Learners](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02220.pdf)                                                                                                                            | ECCV 24'           | [Github](https://github.com/GenjiB/AVSiam)                          |
| 2024 | CAT            | [CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios](https://arxiv.org/pdf/2403.04640)                                                                                                                        | ECCV 24'           | [Github](https://github.com/rikeilong/Bay-CAT)                      |
| 2024 | CrossMAE       | [CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf) | CVPR 24'           | [Github](https://github.com/TonyLianLong/CrossMAE)                  |
| 2024 | EEMC           | [Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes](https://arxiv.org/pdf/2407.10957)                                                                                                                                                                   | ECCV 24'           | [Github](https://github.com/GeWu-Lab/Ref-AVS)                       |
| 2024 | GPT-4o         | [GPT-4o System Card](https://arxiv.org/pdf/2410.21276)                                                                                                                                                                                                          | -                  | [Github](https://github.com/marketplace/models/azure-openai/gpt-4o) |
| 2024 | LAST-Att       | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased](https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf)                                    | WACV 24'           | [Github](https://github.com/DragonLiu1995/MUSIC-AVQA-v2.0)          |
| 2024 | MAVEN          | [FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning](https://arxiv.org/pdf/2504.00487)                                                                                                                           | NIPS 24'           | [Github](https://github.com/reml-group/fortisavqa)                  |
| 2024 | MCCD           | [Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering](https://proceedings.neurips.cc/paper_files/paper/2024/file/123a18dfd821c8b440f42a00a27648d6-Paper-Conference.pdf)                                                             | NIPS 24'           | [Github](https://github.com/reml-group/MUSIC-AVQA-R)                |
| 2024 | Meerkat        | [Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf)                                                                                                                | ECCV 24'           | [Github](https://github.com/schowdhury671/meerkat)                  |
| 2024 | OGM            | [On-the-fly Modulation for Balanced Multimodal Learning](https://arxiv.org/pdf/2410.11582)                                                                                                                                                                      | T-PAMI 24'         | [Github](https://github.com/GeWu-Lab/BML_TPAMI2024)                 |
| 2024 | OneLLM         | [OneLLM: One Framework to Align All Modalities with Language](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf)                                                         | CVPR 24'           | [Github](https://github.com/csuhan/OneLLM)                          |
| 2024 | OPM            | [On-the-fly Modulation for Balanced Multimodal Learning](https://arxiv.org/pdf/2410.11582)                                                                                                                                                                      | T-PAMI 24'         | [Github](https://github.com/GeWu-Lab/BML_TPAMI2024)                 |
| 2024 | QaP            | [Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf)                 | CVPR 24'           | [Github](https://github.com/Rainlt/QaP/)                            |
| 2024 | RefAtomNet     | [Referring Atomic Video Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02873.pdf)                                                                                                                                                 | ECCV 24'           | -                                                                   |
| 2024 | VALOR          | [VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset](https://arxiv.org/pdf/2304.08345)                                                                                                                                                  | T-PAMI 24'         | [Github](https://github.com/TXH-mercury/VALOR)                      |
| 2024 | VideoLLaMA-2   | [VideoLLaMA 2 Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/pdf/2406.07476)                                                                                                                                      | -                  | [Github](https://github.com/DAMO-NLP-SG/VideoLLaMA2)                |
| 2024 | VITA           | [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/pdf/2408.05211)                                                                                                                                                                   | CoRR 24'           | [Github](https://github.com/VITA-MLLM/VITA)                         |
| 2025 | Qwen2.5-VL     | [Qwen2.5-VL Technical Report](https://arxiv.org/pdf/2502.13923)                                                                                                                                                                                                 | -                  | [Github](https://github.com/QwenLM/Qwen2.5-VL)                      |

## Description

The table below illustrates methods incorporating explicit spatial-temporal design components in detail. Each method‚Äôs corresponding paper and code can be found in [&sect; Methods](#methods).
| Method | Brief Description |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Amuse | Focuses on music-performance scenarios by aligning time segments in both audio and video streams via a cross-attention paradigm. Exploits beat-/note-level alignment to model subtle temporal dependencies, aiding questions that involve simultaneous or evolving instrument patterns. |
| AVST | Proposes a spatio-temporal grounded approach that localizes sounding objects per frame and applies question-guided temporal attention. Combines localized visual features with temporal cues for stronger spatial-temporal reasoning. |
| CIGN | Learns audio-visual class tokens and an Audio-Visual Continual Grouping module that clusters spectrogram regions and image patches at every step, tracking objects/sounds across time for temporally consistent reasoning. |
| DCL | Introduces Disentangled Counterfactual Learning: a VAE splits signals into static vs. dynamic factors, and a counterfactual intervention module enables causal reasoning over temporal event relationships. |
| DG-SCT | Dual-Guided Spatial-Channel-Temporal attention layer injected into frozen transformers; bidirectional audio‚Üîvision prompts highlight salient regions, channels, and time segments for fine-grained alignment. |
| EEMC | Segments audio/video into 1 s slices and fuses them with text via a Temporal Bi-modal Transformer with cached memory, magnifying sudden temporal changes for precise evolving-object localisation. |
| LAST-Att | Repeated cross-attention between Swin-Transformer-v2 (vision) and an Audio Spectrogram Transformer; iteratively attends to key frames/patches, localising musical actions over time. |
| LAVisH | Adds Latent Audio-Visual Hybrid adapters to every ViT layer; a pool of latent tokens acts as a cross-attention bottleneck so audio frames gate visual tokens throughout video playback. |
| LAViT | For 360¬∞ videos: augments patches with quaternion-based spherical coords and aligns them with audio via contrastive learning, enabling reasoning about where/when a sound arises on the sphere. |
| LSTTA | Parameter-efficient transfer learning with adapters; splits temporal modelling into short-term semantic interaction and long-term filtering, refining when/how long instruments contribute. |
| MAVEN | Cycles audio, video and text logits with debiasing constraints, anchoring questions to correct temporal segments while suppressing spurious modality correlations. |
| MCCD | Multifaceted Cycle-Collaborative Debiasing: KL penalties widen uni- vs. tri-modal gaps per timestep and force unimodal paths to agree, stabilising spatial-temporal grounding under shift. |
| Meerkat | Two-stage fine-grained grounding: AV Optimal Transport aligns audio with image patches, then AV Attention Consistency refines maps to locate sources within bounding boxes. |
| PSTP-Net | Progressive Spatio-Temporal Perception: (1) Temporal Segment Selection, (2) Spatial Region Selection, (3) Audio-guided Visual Attention isolate question-relevant info step-by-step. |
| RefAtomNet | Three-stream network (visual, text, location-semantic) with agent-attention blocks; location tokens give bounding-box hints over time for tight spatial-temporal coupling of atomic actions. |
| VideoLLaMA-2 | Uses Spatial-Temporal Convolution connector to mix per-frame spatial info then downsample temporally; a synced audio branch injects spectrogram tokens for holistic AV QA. |

Additionally, the other methods without spatial-temporal design are shown below:

| Method         | Brief Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Audio Flamingo | The paper proposes Audio Flamingo, a novel audio language model designed to enhance large language models‚Äô (LLMs) understanding of non-speech sounds and non-verbal speech through three key innovations. It employs a sliding-window audio feature extractor to preserve temporal information in variable-length audio, while cross-attention mechanisms efficiently fuse audio inputs into the LM to reduce computational overhead. The model leverages a curated heterogeneous dataset and a two-stage training approach (pre-training and supervised fine-tuning) to balance close-ended and open-ended tasks. Additionally, it integrates in-context learning (ICL) and retrieval-augmented generation (RAG) through tailored templates and cross-attention masks, enabling few-shot adaptation without fine-tuning. To support multi-turn dialogues, the model is fine-tuned on GPT-4-generated datasets with correlated context. By combining these techniques, Audio Flamingo addresses challenges in audio feature extraction, heterogeneous data training, task adaptation, and dialogue coherence, achieving state-of-the-art performance across tasks. |
| AVMoE          | The paper proposes a parameter-efficient transfer learning framework for audio-visual tasks by dynamically integrating intra-modal and inter-modal information through a mixture of experts. The approach introduces unimodal adapters to capture within-modality details and cross-modal adapters to model interactions between audio-visual streams, while a lightweight modality-agnostic router dynamically allocates expert weights based on input characteristics. By combining these components, AVMoE adaptively balances modality-specific and cross-modal features, addressing challenges like missing modalities or noisy inputs, thereby enhancing robustness and performance across diverse audio-visual tasks such as AV localization, segmentation, and question answering without requiring full model fine-tuning.                                                                                                                                                                                                                                                                                                                                |
| AVSD           | The paper proposes an end-to-end baseline for audio-visual scene-aware dialog to enhance virtual assistants by integrating multimodal signals. The method employs an attention mechanism to differentiate useful signals from distractions, while maintaining spatial features from video frames (VGG19/I3D-Kinetics) to preserve contextual details and temporally subsampling frames to improve efficiency. By fusing attended vectors across audio, video, and text modalities, the approach dynamically focuses on relevant cues during answer generation. This integrated framework addresses challenges in holistic dialog management, leveraging cross-modal interactions to outperform prior methods without relying on rigid pipelines, as demonstrated on the audio-visual scene-aware dataset.                                                                                                                                                                                                                                                                                                                                                          |
| AVSiam         | The paper proposes an efficient and scalable audio-visual learning framework using a shared vision transformer backbone to unify audio and visual processing. The AVSiam model employs a contrastive audio-visual matching objective with a multi-ratio random masking scheme to enhance representation robustness while enabling larger batch sizes for effective contrastive learning. By sharing parameters across modalities, the approach reduces GPU memory footprint and computational costs compared to dual-backbone methods, while maintaining competitive performance on classification and retrieval tasks. This integrated design addresses scalability challenges and modality-handling flexibility without compromising accuracy.                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Att-BLSTM      | The paper proposes an attention-based bidirectional LSTM network (Att-BLSTM) for relation classification to capture decisive semantic information without relying on lexical resources or NLP systems. The model processes raw text through an embedding layer to generate word vectors, while bidirectional LSTM (BLSTM) layers learn high-level features by incorporating both past and future context. An attention mechanism then assigns weights to key words, merging word-level features into a sentence-level vector for classification. By integrating these components, the approach overcomes limitations of manual feature engineering and dependency on external tools, effectively identifying critical semantic cues across sentence positions to improve relation classification performance.                                                                                                                                                                                                                                                                                                                                                      |
| CAT            | The paper proposes an enhanced Multimodal Large Language Model (MLLM) to improve question answering in dynamic audio-visual scenarios by addressing ambiguity and localization challenges. Key components include a clue aggregator to dynamically capture question-aware audio-visual features for fine-grained grounding, a mixed training strategy combining video-text and audio-text pairs with a novel AVinstruct dataset to strengthen cross-modal awareness, and an AI-assisted Ambiguity-aware Direct Preference Optimization (ADPO) to retrain the model for precise responses. By integrating these innovations, CAT effectively mitigates ambiguous outputs and enhances audio-visual reasoning, outperforming existing methods in Audio-Visual Question Answering (AVQA) tasks.                                                                                                                                                                                                                                                                                                                                                                       |
| ChatBridge     | The paper proposes a multimodal language model that leverages large language models (LLMs) as a universal interface to bridge diverse modalities through language-paired data. ChatBridge integrates modality-specific encoders and perceiver modules to project embeddings into the LLM‚Äôs semantic space, enabling cross-modal correlation without requiring all paired data combinations. The model undergoes two-stage training: first aligning modalities with language to emergent multimodal abilities, then instruction-fine-tuning on the MULTIS dataset to enhance zero-shot task generalization. By using language as a catalyst, ChatBridge addresses the challenge of limited multimodal paired data while achieving strong performance across text, image, video, and audio tasks through unified multimodal reasoning and user intent alignment.                                                                                                                                                                                                                                                                                                     |
| COCA           | The paper proposes a collaborative causal regularization framework (COCA) to address multi-shortcut biases in Audio-Visual Question Answering (AVQA) by integrating causal intervention and dynamic debiasing. The Bias-centered Causal Regularization (BCR) mitigates specific shortcut biases (Q‚ÜíG, V&Q‚ÜíG, A&Q‚ÜíG) through counterfactual interventions to disrupt bias-irrelevant causal effects and factual regularization to maintain semantic consistency, while the Multi-shortcut Collaborative Debiasing (MCD) dynamically adjusts debiasing focus per sample using an entropy-driven metric to balance bias contributions. By jointly addressing uni-modal and joint-modal biases through causal introspection and instance-aware adaptation, COCA enhances multimodal reasoning robustness without over-correcting, achieving state-of-the-art performance on MUSIC-AVQA.                                                                                                                                                                                                                                                                                |
| CONVLSTM       | The paper proposes a novel approach to enhance temporal reasoning in Audio Question Answering (AQA) by introducing the Diagnostic Audio Question Answering (DAQA) dataset, which comprises natural sound events and programmatically generated questions to probe temporal reasoning skills, while adapting visual question answering methods to AQA reveals their limitations. To address this, the authors develop Multiple Auxiliary Controllers for Linear Modulation (MALiMo), which extends Feature-wise Linear Modulation (FiLM) by incorporating an additional auxiliary controller to process subsampled audio features, thereby enabling dynamic modulation of convolutional network processing based on both input modalities. This integrated approach improves relational and temporal reasoning by jointly leveraging audio and question inputs, overcoming the shortcomings of existing methods in handling complex temporal dependencies within sound sequences.                                                                                                                                                                                   |
| CrossMAE       | The paper proposes a region-aware audio-visual pre-training framework to enhance cross-modality interaction and fine-grained alignment by extending masked autoencoders. It introduces Cross-Conditioned Reconstruction to reconstruct input pixels conditioned on cross-modal Attentive Tokens, while Cross-Embedding Reconstruction leverages Learnable Queries with positional cues to guide feature reconstruction between modalities, supplemented by contrastive loss for global alignment. By integrating these components, CrossMAE addresses the limitations of prior global feature-based methods, enabling effective region-level understanding and improving performance in both classification and dense prediction tasks without task-specific fine-tuning.                                                                                                                                                                                                                                                                                                                                                                                          |
| FCNLSTM        | The paper proposes a novel approach to enhance temporal reasoning in Audio Question Answering (AQA) by introducing the Diagnostic Audio Question Answering (DAQA) dataset, which comprises natural sound events and programmatically generated questions to probe temporal reasoning skills. While adapting existing visual question answering methods to AQA reveals their limitations in temporal reasoning, the authors develop Multiple Auxiliary Controllers for Linear Modulation (MALiMo) to extend Feature-wise Linear Modulation (FiLM) by incorporating an additional auxiliary controller to process subsampled audio features, thereby enabling dynamic modulation of convolutional network processing based on both principal and supplementary inputs. This integrated approach addresses the challenge of in-depth temporal reasoning by facilitating relational and temporal analysis, leading to improved performance on DAQA without relying on spatial reasoning or static inputs.                                                                                                                                                              |
| GPT-4o         | The paper proposes GPT-4o, an autoregressive omni model designed to process any combination of text, audio, image, and video inputs while generating text, audio, or image outputs through end-to-end training across modalities. By integrating Web Data, Code and Math, and Multimodal Data during pre-training, the model learns diverse reasoning skills and multimodal interpretation, while post-training alignment and red-teaming mitigate risks such as bias and harmful content. This unified approach enhances real-time responsiveness, multilingual performance, and multimodal understanding while addressing safety concerns through layered mitigations and external evaluations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| GRU            | The paper proposes a free-form, open-ended Visual Question Answering (VQA) task to generate natural language answers from images and questions, mirroring real-world scenarios like assisting the visually impaired. The approach leverages a large dataset (0.25 M images, 0.76 M questions, 10 M answers) combining real images from MS COCO and abstract scenes to enable both low-level vision and high-level reasoning. By supporting diverse question types (e.g., fine-grained recognition, commonsense reasoning) and offering automatic evaluation through open-ended or multiple-choice formats, the framework addresses the need for detailed image understanding and multi-modal knowledge integration, advancing AI-complete challenges beyond generic captioning.                                                                                                                                                                                                                                                                                                                                                                                    |
| HCRN           | The paper proposes a general-purpose neural unit for video question answering that enables hierarchical relational reasoning and multimodal fusion. The Conditional Relation Network (CRN) processes input object arrays through sparse high-order relations while modulating encodings with contextual features, allowing flexible replication and stacking into Hierarchical CRNs (HCRN). The architecture integrates appearance features with clip motion as initial context, then progressively incorporates linguistic context and video-level motion through layered CRNs to enable multi-step reasoning. By hierarchically combining localized clip relations with global video and question contexts, HCRN addresses challenges of modeling distant temporal dependencies and heterogeneous modalities in VideoQA, demonstrating robust performance across diverse question types requiring appearance, motion, and temporal reasoning.                                                                                                                                                                                                                    |
| HCAttn         | The paper proposes a hierarchical co-attention model for Visual Question Answering (VQA) that jointly reasons about image and question attention to improve answer accuracy. It introduces a co-attention mechanism to simultaneously perform question-guided visual attention (to identify relevant image regions) and image-guided question attention (to focus on key words), while employing a hierarchical question representation through word-level embeddings, phrase-level 1D CNNs (to capture n-gram features), and question-level LSTMs (to encode contextual meaning). By recursively combining co-attended features across these levels, the model addresses challenges like linguistic variation and multi-modal alignment, enhancing robustness and fine-grained understanding for VQA tasks.                                                                                                                                                                                                                                                                                                                                                       |
| HME            | The paper proposes a novel VideoQA framework that integrates heterogeneous memory and multimodal attention to enhance video-question reasoning. It introduces a heterogeneous memory module to jointly learn global context from appearance and motion features through synchronized attention, while a redesigned question memory captures complex semantics and highlights queried subjects by storing global contexts. These components interact through a multimodal fusion layer that aligns visual and textual hints via self-updated attention, enabling multi-step reasoning. By unifying feature integration with attention learning and maintaining global context throughout, the approach addresses challenges of spatiotemporal alignment and complex question semantics, improving VideoQA performance without separating feature and attention steps.                                                                                                                                                                                                                                                                                               |
| MCAN           | The paper proposes a deep Modular Co-Attention Network (MCAN) to enhance visual question answering (VQA) by jointly modeling intra- and inter-modal interactions through a modular architecture. The framework integrates Self-Attention (SA) units to capture dense word-to-word and region-to-region relationships within questions and images, while Guided-Attention (GA) units model word-to-region cross-modal dependencies. By cascading Modular Co-Attention (MCA) layers composed of SA and GA units, MCAN enables deep reasoning while addressing the limitations of shallow co-attention models. This integrated approach improves fine-grained semantic understanding by simultaneously refining self-attention within modalities and guided-attention across modalities, leading to more accurate visual-textual alignment and robust performance on complex VQA tasks.                                                                                                                                                                                                                                                                               |
| OGM & OPM      | The paper proposes an adaptive modulation approach to address imbalanced multimodal learning by dynamically balancing uni-modal optimization during joint training. It introduces On-the-fly Prediction Modulation (OPM) to weaken dominant modality influence in the feed-forward stage by probabilistically dropping its features, while On-the-fly Gradient Modulation (OGM) mitigates gradient dominance in back-propagation through adaptive noise injection. By monitoring inter-modal discriminative discrepancies, these strategies jointly alleviate under-optimization of weaker modalities while preserving dominant modality contributions. The integrated framework enhances multimodal representation learning across diverse tasks by ensuring balanced feature optimization without additional training overhead, as validated through extensive experiments on audio-visual benchmarks.                                                                                                                                                                                                                                                           |
| OneLLM         | The paper proposes a unified framework to align multiple modalities with language using a shared architecture, eliminating the need for modality-specific encoders. It introduces lightweight modality tokenizers to convert input signals into tokens, while a universal encoder (CLIP-ViT) extracts cross-modal features and a universal projection module (UPM) dynamically routes mixed projection experts to map diverse modalities into the LLM's embedding space. Through progressive alignment and a curated multimodal instruction dataset spanning eight modalities, the integrated approach overcomes scalability limitations of prior MLLMs by unifying encoding and projection, enabling flexible modality expansion and enhanced multimodal understanding without architectural redundancy.                                                                                                                                                                                                                                                                                                                                                          |
| PSAC           | The paper proposes a novel self-attention-based architecture for video question answering (VQA) to overcome the limitations of RNNs in modeling long-range dependencies and parallel processing. It introduces Positional Self-Attention (PSA) to capture global dependencies in video and question sequences by attending to all positions while incorporating absolute positional encodings to preserve temporal/spatial information. Through Video-based PSA (VPSA) and Question-based PSA (QPSA), the model encodes video frames and textual questions in parallel. A Video-Question Co-Attention (VQ-Co) block then simultaneously attends to relevant visual and textual features via bidirectional attention, enhancing cross-modal alignment. By integrating PSA with co-attention, the framework efficiently models complex video-question interactions, addressing challenges in sequential data processing and multimodal fusion while improving accuracy and computational efficiency.                                                                                                                                                                 |
| QaP            | The paper proposes a parameter-efficient multimodal language model learning strategy that bridges modalities through query-based prompts and lightweight resampling. The core innovation involves Querying Prompts (QP) to simultaneously extract modality information and interact with text, while Text-Conditioned Resamplers (TCR) adaptively inject text-relevant multimodal features into frozen language model layers. By integrating QP and TCR, the approach efficiently compresses modality inputs and leverages the model's inherent fusion capabilities, addressing computational inefficiency and redundancy in traditional projection-based methods while outperforming existing techniques across multiple multimodal tasks with minimal trainable parameters.                                                                                                                                                                                                                                                                                                                                                                                      |
| Qwen2.5-VL     | The paper proposes Qwen2.5-VL, a vision-language model advancing multimodal understanding through enhanced visual recognition, object localization, and document parsing while addressing computational and contextual challenges. Key innovations include dynamic resolution processing to handle varying image sizes and video durations, absolute time encoding to improve temporal dynamics perception, and a native dynamic-resolution ViT with Window Attention to reduce overhead while preserving resolution. By integrating these components, the model achieves robust performance in fine-grained visual tasks, long-video comprehension, and real-world agentic applications without task-specific fine-tuning, while maintaining strong linguistic capabilities inherited from Qwen2.5 LLM. The approach overcomes bottlenecks in computational complexity and inconsistent sequence-length performance, enabling precise spatial-temporal reasoning and cross-domain generalization.                                                                                                                                                                 |
| VALOR          | The paper proposes a Vision-Audio-Language Omni-Perception pretraining model (VALOR) to jointly model tri-modality interactions for understanding and generation tasks. It employs three single-modality encoders to process vision, audio, and language separately, while a multimodal decoder enables conditional text generation through two pretext tasks: Multimodal Grouping Alignment (MGA) projects modalities into a shared space to align vision-language, audio-language, and audiovisual-language groups via contrastive learning, and Multimodal Grouping Captioning (MGC) reconstructs masked text tokens conditioned on visual, auditory, or combined inputs to enhance generative capabilities. By integrating these components with a large-scale human-annotated dataset (VALOR-1M), the approach addresses the limitations of existing bimodal systems, enabling comprehensive cross-modal alignment and flexible text generation across diverse modality combinations for downstream tasks like retrieval, captioning, and question answering.                                                                                                 |
| VAST           | The paper proposes an omni-modality foundation model to enhance video-text cross-modality learning by integrating vision, audio, and subtitle information. It introduces VAST-27M, a large-scale dataset automatically generated through a two-stage pipeline: first training separate vision and audio captioners to produce single-modality descriptions, then employing an LLM to synthesize these with subtitles into omni-modality captions. The VAST model leverages three modality encoders and cross-attention-based text fusion, trained with objectives (OM-VCC/VCM/VCG) to unify multi-modal understanding. This approach addresses the lack of comprehensive video-text corpora by automating caption generation, enabling joint modeling of complementary modalities to improve performance on diverse downstream tasks like retrieval, captioning, and QA without manual annotation costs.                                                                                                                                                                                                                                                           |
| VITA           | The paper proposes VITA, an open-source Multimodal Large Language Model (MLLM) capable of simultaneous processing and interactive analysis across video, image, text, and audio modalities. Starting with Mixtral 8√ó7B as a language foundation, it expands Chinese vocabulary through bilingual instruction tuning to enhance multilingual proficiency, while endowing visual and audio capabilities via two-stage multi-task learning for multimodal alignment and instruction tuning. To improve interaction, VITA introduces state tokens to distinguish input queries for non-awakening interaction and employs a duplex pipeline deployment scheme, where one model generates responses while another monitors environmental inputs, enabling audio interrupt interaction. This integrated approach addresses the lack of open-source models with unified multimodal processing and natural interaction, advancing seamless multimodal understanding and human-computer engagement without relying on wake-up words or sequential query handling.                                                                                                            |
